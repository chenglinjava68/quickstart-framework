1、消息组件msgframe：RocketMQ、ActiveMQ、kafka

2、缓存aicache：Redis、Memecached

3、amber配置中心

4、微服务治理框架CSF：RPC框架


5、

6、

7、

8、

9、

10、

11、

12、

13、

14、

15、

16、

17、

18、

19、

20、

21、

22、

23、

24、

25、



---------------------------------------------------------------------------------------------------------------------
消息组件msgframe：

异步解耦，削峰填谷

1、统一接口、对象，屏蔽差异
2、配置的动态刷新（console、amber、local）
3、连接池：对象、连接统一管控，动态调整（线程池）
4、主题的分片，发送负载路由（提升并发TPS）
5、消费端去重
6、

同步、异步发送回调、顺序、事务、事务的统一提交
发送客户端的限流（TTL动态批量统计）

消息序列化提供：(Kryo和FST)
消息的跟踪（ES,Jest(searchbox)作为客户端查询）
消息对象使用对象池的技术
连接池的使用

客户端：SDK方式提供jar，屏蔽不同消息中间件差异，修复完善功能、定制开发
Console：监控、运维、配置

1、屏蔽不同消息中间件差异：发送、消费接口，队列负载均衡，主题队列模型，集群模型等
2、定制开发：消息压缩，消息加解密，消息轨迹跟踪、消息稽核、监控告警、消息去重、broker故障隔离、消息异常（生产、消费）的持久化、消息分析（业务端开发）
	权限、流控、黑白名单等
	熔断：broker积压之后熔断，消费端消费的mq积压之后，熔断退出由其他消费者消费
3、Console：监控、运维、配置

消息类型：
从通讯方式：
同步、异步、oneway

顺序消息
事务消息
延时消息（定时消息）
消息优先级
tag消息

消息选型比较：
功能性、TPS、积压、
社区、文档、语言、持久化、事务、负载均衡、扩缩容、部署、维护

消费端的去重（Redis，setnx命令）

Console：
配置
监控告警
租户

遇到问题：
1、mq版本5.13.2，是3台mq配合zookeeper组成的集群
出现异常之前手动停止了mq集群，停止的时候有生产者在生产消息
启动集群后，有脏数据的mq的leveldb就同步到了整个集群，导致一只出现主从切换，只能删除所有leveldb才能恢复

1、Rocketmq的队列大小扩缩容：缩容时候，导致部分数据无法消费
2、consumer设置上instanceName后，无法集群消费的问题调查：以为是因为设置的广播消费模式，发现不是，原来是如果设置上这个参数，启动多个jvm进程，则currentCID都一样，而计算rebalance时如下代码导致每次将所有的queue分配到一个consumer上：
int index = cidAll.indexOf(currentCID);
3、Rocketmq的发送消息，同步发送，设置同步刷盘，但是刷盘要保存的commitlog被删除，也会返回成功。


事务消息错误使用ThreadLocal导致事务消息一直发送到同一个broker上


线程池参数设置的不合理等导致oom



---------------------------------------------------------------------------------------------------------------------
amber配置中心：

Client启动的时候通过配置注解或者直接注入回调类方式
Client请求配置中心服务（内部的注册中心保证配置中心高可用），获取配置，并放入Redis缓存。
在amber配置中心更新配置的时候，使用ZK（CenterCode+AppCode+Key）通知客户端配置有变化，客户端根据ZK的通知去Redis缓存或者amber配置中心服务（数据库）获取配置
更新配置：1、更新到数据，2、刷新Redis缓存，3、Zk通知客户端

消息地址在Client通过获取配置的时候获取，客户端不需要知道，后面消息地址可以随时修改
改进：配置中心做Redis缓存，第一次启动从配置中心获取，配置中心缓存到Redis缓存，配置修改通过消息通知，Cient做持久化和内存两种缓存。
Client启动的时候，首先从配置中心，再到本地获取配置

---------------------------------------------------------------------------------------------------------------------
微服务治理框架CSF：RPC框架

Provider和Consumer都从Zookeeper获取服务路由信息，
1、  使用Zookeeper做服务注册中心
2、实现服务间的调用、负载均衡、降级、熔断、重试、限流、跟踪等机制
负载均衡：轮训、随机、一致性哈希、最小连接数，加权轮训、加权随机等
超时控制等降级：报错，降级函数
熔断：通过Zookeeper通知熔断
失败重试
通过控制并发线程数的Semaphore，控制并发
跟踪使用log4X

微服务：服务熔断、降级、限流
服务端做、客户端做
服务熔断、降级：超时、错误、请求并发数过大
限流手法：线程数+队列，信号量、令牌桶算法Guava的RateLimiter



---------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------

---------------------------------------------------------------------------------------------------------------------



